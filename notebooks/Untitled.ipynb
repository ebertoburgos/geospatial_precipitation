{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc120d22-07ad-4eb0-82fc-5b115c30cf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Reading gcp-public-data-arco-era5/raw/date-variable-single_level/2022/12/01/total_precipitation/surface.nc...\n",
      "Number of rows: 24917760\n",
      "- Creating Spark DataFrame...\n",
      "Number of rows after adding index: 10000\n",
      "- Writing output...\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.types import StringType\n",
    "import h3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PrecipitationApp\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/home/jovyan/work/spark-warehouse\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#    .config(\"spark.driver.memory\", \"0.5g\") \\\n",
    "#    .config(\"spark.executor.memory\", \"0.5g\") \\\n",
    "#    .config(\"spark.executor.cores\", \"1\") \\\n",
    "\n",
    "# Path to the files in GCS\n",
    "file_path_pattern = \"gs://gcp-public-data-arco-era5/raw/date-variable-single_level/2022/12/01/total_precipitation/surface.nc\"\n",
    "\n",
    "# Use gcsfs to open the files directly from GCS\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "file_paths = fs.glob(file_path_pattern)\n",
    "\n",
    "# Initialize empty list to store DataFrames\n",
    "data_frames = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    print(f\"- Reading {file_path}...\")\n",
    "    with fs.open(file_path, 'rb') as f:\n",
    "        data = xr.open_dataset(f, engine='scipy')\n",
    "    \n",
    "    lats = data['latitude'].values\n",
    "    lons = data['longitude'].values\n",
    "    precipitation = data['tp'].values\n",
    "    time = data['time'].values\n",
    "\n",
    "    lats_flat = np.repeat(lats, len(lons) * len(time))\n",
    "    lons_flat = np.tile(np.repeat(lons, len(time)), len(lats))\n",
    "    precipitation_flat = precipitation.flatten()\n",
    "    time_flat = np.tile(time, len(lats) * len(lons))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'latitude': lats_flat,\n",
    "        'longitude': lons_flat,\n",
    "        'timestamp': pd.to_datetime(time_flat),\n",
    "        'precipitation': precipitation_flat\n",
    "    })\n",
    "    # Count the number of rows\n",
    "    print(f\"Number of rows: {df.shape[0]}\")\n",
    "\n",
    "    df = df.head(10000)\n",
    "    \n",
    "    print(\"- Creating Spark DataFrame...\")\n",
    "    df_spark = spark.createDataFrame(df)\n",
    "    df_spark = df_spark.withColumn('date', to_date(col('timestamp')))\n",
    "\n",
    "    # Define UDF to convert lat/lon to H3 index\n",
    "    def lat_lon_to_h3(lat, lon, resolution):\n",
    "        return h3.geo_to_h3(lat, lon, resolution)\n",
    "\n",
    "    h3_udf = udf(lambda lat, lon: lat_lon_to_h3(lat, lon, resolution=3), StringType())\n",
    "\n",
    "    # Add H3 index to Spark DataFrame\n",
    "    df_h3 = df_spark.withColumn('h3_index', h3_udf(col('latitude'), col('longitude')))\n",
    "\n",
    "    # Count the number of rows\n",
    "    print(f\"Number of rows after adding index: {df_h3.count()}\")\n",
    "\n",
    "    # Save the result to a parquet file\n",
    "    print(\"- Writing output...\")\n",
    "    df_h3.write.mode(\"overwrite\").partitionBy(\"date\", \"h3_index\").parquet(\"/home/jovyan/work/data/precipitation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4f1bbb-fcba-42e0-a0d8-fbb9351b6c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|David| 35|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create or retrieve a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Test App\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define a list of tuples containing sample data\n",
    "data = [\n",
    "    (\"Alice\", 34),\n",
    "    (\"Bob\", 45),\n",
    "    (\"Cathy\", 29),\n",
    "    (\"David\", 35)\n",
    "]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "# Create a DataFrame from the sample data and schema\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Perform a basic transformation: filter out people older than 30\n",
    "filtered_df = df.filter(col(\"Age\") > 30)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "filtered_df.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f614b-b1ff-48bc-b27f-ae8a1f5698c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
