{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc120d22-07ad-4eb0-82fc-5b115c30cf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: timed out\n",
      "WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused\n",
      "WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused\n",
      "WARNING:google.auth._default:Authentication failed using Compute Engine authentication due to unavailable metadata server.\n",
      "WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 1 of 5. Reason: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/service-accounts/default/?recursive=true (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0xffff2f8a1810>: Failed to resolve 'metadata.google.internal' ([Errno -2] Name or service not known)\"))\n",
      "WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 2 of 5. Reason: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/service-accounts/default/?recursive=true (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0xffff7114d5d0>: Failed to resolve 'metadata.google.internal' ([Errno -2] Name or service not known)\"))\n",
      "WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 3 of 5. Reason: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/service-accounts/default/?recursive=true (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0xffff7114e690>: Failed to resolve 'metadata.google.internal' ([Errno -2] Name or service not known)\"))\n",
      "WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 4 of 5. Reason: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/service-accounts/default/?recursive=true (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0xffff7114fa90>: Failed to resolve 'metadata.google.internal' ([Errno -2] Name or service not known)\"))\n",
      "WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 5 of 5. Reason: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/service-accounts/default/?recursive=true (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0xffff71158fd0>: Failed to resolve 'metadata.google.internal' ([Errno -2] Name or service not known)\"))\n",
      "INFO:__main__:Reading gcp-public-data-arco-era5/raw/date-variable-single_level/2022/12/01/total_precipitation/surface.nc...\n",
      "INFO:__main__:Number of rows: 24917760\n",
      "INFO:__main__:- Creating Spark DataFrame...\n",
      "INFO:__main__:Number of rows after adding index: 10000\n",
      "INFO:__main__:- Writing output...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import h3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, to_date\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Constants\n",
    "FILE_PATH_PATTERN = \"gs://gcp-public-data-arco-era5/raw/date-variable-single_level/2022/12/01/total_precipitation/surface.nc\"\n",
    "PARQUET_OUTPUT_PATH = \"/home/jovyan/work/data/precipitation\"\n",
    "H3_RESOLUTION = 3\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def initialize_spark_session():\n",
    "    \"\"\"Initialize and return a Spark session.\"\"\"\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"PrecipitationApp\") \\\n",
    "        .config(\"spark.master\", \"local[*]\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/home/jovyan/work/spark-warehouse\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "        .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def read_data_from_gcs(file_path_pattern):\n",
    "    \"\"\"Read data files from GCS using gcsfs.\"\"\"\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    file_paths = fs.glob(file_path_pattern)\n",
    "    return file_paths\n",
    "\n",
    "def process_file(file_path, fs):\n",
    "    \"\"\"Process a single file and return a pandas DataFrame.\"\"\"\n",
    "    logger.info(f\"Reading {file_path}...\")\n",
    "    with fs.open(file_path, 'rb') as f:\n",
    "        data = xr.open_dataset(f, engine='scipy')\n",
    "    \n",
    "    lats = data['latitude'].values\n",
    "    lons = data['longitude'].values\n",
    "    precipitation = data['tp'].values\n",
    "    time = data['time'].values\n",
    "\n",
    "    lats_flat = np.repeat(lats, len(lons) * len(time))\n",
    "    lons_flat = np.tile(np.repeat(lons, len(time)), len(lats))\n",
    "    precipitation_flat = precipitation.flatten()\n",
    "    time_flat = np.tile(time, len(lats) * len(lons))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'latitude': lats_flat,\n",
    "        'longitude': lons_flat,\n",
    "        'timestamp': pd.to_datetime(time_flat),\n",
    "        'precipitation': precipitation_flat\n",
    "    })\n",
    "    \n",
    "    logger.info(f\"Number of rows: {df.shape[0]}\")\n",
    "    return df.head(10000)  # Limit to 10,000 rows\n",
    "\n",
    "def create_spark_dataframe(df, spark):\n",
    "    \"\"\"Convert a pandas DataFrame to a Spark DataFrame and add a date column.\"\"\"\n",
    "    logger.info(\"- Creating Spark DataFrame...\")\n",
    "    df_spark = spark.createDataFrame(df)\n",
    "    return df_spark.withColumn('date', to_date(col('timestamp')))\n",
    "\n",
    "def lat_lon_to_h3(lat, lon, resolution):\n",
    "    \"\"\"Convert latitude and longitude to H3 index.\"\"\"\n",
    "    return h3.geo_to_h3(lat, lon, resolution)\n",
    "\n",
    "def add_h3_index(df_spark):\n",
    "    \"\"\"Add H3 index to the Spark DataFrame.\"\"\"\n",
    "    h3_udf = udf(lambda lat, lon: lat_lon_to_h3(lat, lon, H3_RESOLUTION), StringType())\n",
    "    return df_spark.withColumn('h3_index', h3_udf(col('latitude'), col('longitude')))\n",
    "\n",
    "def write_to_parquet(df_h3):\n",
    "    \"\"\"Write the Spark DataFrame to a parquet file.\"\"\"\n",
    "    logger.info(\"- Writing output...\")\n",
    "    df_h3.write.mode(\"overwrite\").partitionBy(\"date\", \"h3_index\").parquet(PARQUET_OUTPUT_PATH)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the ETL pipeline.\"\"\"\n",
    "    spark = initialize_spark_session()\n",
    "    file_paths = read_data_from_gcs(FILE_PATH_PATTERN)\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        df = process_file(file_path, fs)\n",
    "        df_spark = create_spark_dataframe(df, spark)\n",
    "        df_h3 = add_h3_index(df_spark)\n",
    "        \n",
    "        logger.info(f\"Number of rows after adding index: {df_h3.count()}\")\n",
    "        write_to_parquet(df_h3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4f1bbb-fcba-42e0-a0d8-fbb9351b6c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "|David| 35|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create or retrieve a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Test App\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define a list of tuples containing sample data\n",
    "data = [\n",
    "    (\"Alice\", 34),\n",
    "    (\"Bob\", 45),\n",
    "    (\"Cathy\", 29),\n",
    "    (\"David\", 35)\n",
    "]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "# Create a DataFrame from the sample data and schema\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Perform a basic transformation: filter out people older than 30\n",
    "filtered_df = df.filter(col(\"Age\") > 30)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "filtered_df.show()\n",
    "\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f614b-b1ff-48bc-b27f-ae8a1f5698c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
