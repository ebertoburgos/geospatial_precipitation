version: '3.8'

services:
  hive-metastore:
    image: bde2020/hive-metastore-postgresql:2.3.0
    #platform: linux/amr64
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
    networks:
      - spark-network

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    #platform: linux/amr64
    environment:
      SERVICE_PRECONDITION: hive-metastore:5432
      HIVE_METASTORE_URI: thrift://hive-metastore:9083
    ports:
      - "10000:10000"
    networks:
      - spark-network
    depends_on:
      - hive-metastore
    volumes:
      - ./data/warehouse:/user/hive/warehouse

  spark-master:
    image: bde2020/spark-master:3.1.2-hadoop3.2
    #platform: linux/amr64
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - PATH=$PATH:/spark/bin
    ports:
      - "8080:8080"
    networks:
      - spark-network
    command: >
      bash -c "pip install -r requirements.txt"
    volumes:
      - ./data:/data
      - ./spark/scripts:/opt/spark/scripts
      - ./requirements.txt:/requirements.txt

  spark-worker:
    image: bde2020/spark-worker:3.1.2-hadoop3.2
    #platform: linux/amr64
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    networks:
      - spark-network
    depends_on:
      - spark-master
    command: >
      bash -c "pip install -r requirements.txt"
    volumes:
      - ./data:/data
      - ./spark/scripts:/opt/spark/scripts
      - ./requirements.txt:/requirements.txt

  airflow-db:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    networks:
      - spark-network
    volumes:
      - ./data/airflow_db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      retries: 5

  airflow:
    image: apache/airflow:2.5.0
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: 'YOUR_FERNET_KEY'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
    ports:
      - "8081:8080"
    networks:
      - spark-network
    depends_on:
      - airflow-db
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./spark/scripts:/opt/spark/scripts
      - ./airflow/logs:/opt/airflow/logs
      - ./requirements.txt:/opt/airflow/requirements.txt
      - /var/run/docker.sock:/var/run/docker.sock  # Mount Docker socket

    command: >
      bash -c "
      while !</dev/tcp/airflow-db/5432; do
        echo 'Waiting for PostgreSQL...';
        sleep 5;
      done;
      airflow db upgrade;
      pip install -r /opt/airflow/requirements.txt;
      airflow users create -r Admin -u admin -e admin@example.com -f admin -l admin -p admin;
      airflow webserver"
    restart: always

  airflow-scheduler:
    image: apache/airflow:2.5.0
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: 'YOUR_FERNET_KEY'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
    networks:
      - spark-network
    depends_on:
      - airflow-db
      - airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./spark/scripts:/opt/spark/scripts
      - ./airflow/logs:/opt/airflow/logs
      - ./requirements.txt:/opt/airflow/requirements.txt
      - /var/run/docker.sock:/var/run/docker.sock  # Mount Docker socket
    command: >
      bash -c "
      while !</dev/tcp/airflow-db/5432; do
        echo 'Waiting for PostgreSQL...';
        sleep 5;
      done;
      pip install -r /opt/airflow/requirements.txt;
      airflow scheduler"
    restart: always
  
  jupyter:
    image: jupyter/pyspark-notebook:latest
    #platform: linux/amr64
    ports:
      - "8888:8888"
    networks:
      - spark-network
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./data:/data
      - ./spark/scripts:/home/jovyan/work
      - ./requirements.txt:/home/jovyan/work/requirements.txt
    command: >
      bash -c "pip install -r /home/jovyan/work/requirements.txt &&
               start-notebook.sh --NotebookApp.token=''"
    depends_on:
      - spark-master

networks:
  spark-network:
    driver: bridge
